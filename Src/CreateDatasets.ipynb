{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1e3b852",
   "metadata": {},
   "source": [
    "# Dataset creation from API notebook\n",
    "The purpose of this notebook is to use the Pageviews API  \n",
    "from https://wikitech.wikimedia.org/wiki/Analytics/AQS/Pageviews  \n",
    "to create three datasets of url traffic data from July 2015 through the previous complete month.  \n",
    "The pages that will be considered for these datasets are located in /Data/ArticleNames.csv.  \n",
    "\n",
    "The three datasets are described to be:  \n",
    "```\n",
    "Dataset1: Get all monthly mobile pageviews per article\n",
    "Dataset2: Get all monthly desktop pageviews per article\n",
    "Dataset3: Get all monthly desktop+mobile pageviews per article\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9099326f",
   "metadata": {},
   "source": [
    "# Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99f456e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import time\n",
    "import urllib.parse\n",
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180121d8",
   "metadata": {},
   "source": [
    "# Load in article data that we want to process into list of lists\n",
    "Element 1: Article title\n",
    "Element 2: Article URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87c5b18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Everything Everywhere All at Once', 'https://en.wikipedia.org/wiki/Everything_Everywhere_All_at_Once']\n",
      "['All Quiet on the Western Front (2022 film)', 'https://en.wikipedia.org/wiki/All_Quiet_on_the_Western_Front_(2022_film)']\n",
      "['The Whale (2022 film)', 'https://en.wikipedia.org/wiki/The_Whale_(2022_film)']\n",
      "Number of articles, should be 1359: 1359\n"
     ]
    }
   ],
   "source": [
    "#Open csv and put data into list of lists\n",
    "ArticleInfo = []\n",
    "with open('../Data/ArticleNames.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    for row in reader:\n",
    "        ArticleInfo.append([row[0], row[1]])\n",
    "\n",
    "#Remove header row\n",
    "ArticleInfo.pop(0)\n",
    "\n",
    "#Print out first few results as proof it is working\n",
    "for i in range(3):\n",
    "    print(ArticleInfo[i])\n",
    "\n",
    "#Show number of articles\n",
    "print(\"Number of articles, should be 1359:\", len(ArticleInfo))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd903ebb",
   "metadata": {},
   "source": [
    "# Helper functions and variables for using the Pageviews API\n",
    "\n",
    "The code in this cell was developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/). Revision 1.2 - August 14, 2023. Some of the original code made be modified, however it still falls under the Creative Commons license."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8151f65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are CONSTANTS used by the API that are relevant to all subsequent API calls\n",
    "# The REST API 'pageviews' URL - this is the common URL/endpoint for all 'pageviews' API requests\n",
    "API_REQUEST_PAGEVIEWS_ENDPOINT = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/'\n",
    "\n",
    "# This is a parameterized string that specifies what kind of pageviews request we are going to make\n",
    "# In this case it will be a 'per-article' based request. The string is a format string so that we can\n",
    "# replace each parameter with an appropriate value before making the request\n",
    "API_REQUEST_PER_ARTICLE_PARAMS = 'per-article/{project}/{access}/{agent}/{article}/{granularity}/{start}/{end}'\n",
    "\n",
    "# The Pageviews API asks that we not exceed 100 requests per second, we add a small delay to each request\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making a request to the Wikimedia API they ask that you include your email address which will allow them\n",
    "# to contact you if something happens - such as - your code exceeding rate limits - or some other error \n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<zbowyer@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2023',\n",
    "}\n",
    "\n",
    "# This template is used to map parameter values into the API_REQUST_PER_ARTICLE_PARAMS portion of an API request. The dictionary has a\n",
    "# field/key for each of the required parameters. In the example, below, we only vary the article name, so the majority of the fields\n",
    "# can stay constant for each request. Of course, these values *could* be changed if necessary.\n",
    "ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE = {\n",
    "    \"project\":     \"en.wikipedia.org\",\n",
    "    \"access\":      \"desktop\",      # this should be changed for the different access types\n",
    "    \"agent\":       \"user\",\n",
    "    \"article\":     \"\",             # this value will be set/changed before each request\n",
    "    \"granularity\": \"monthly\",\n",
    "    \"start\":       \"2015010100\",   # start and end dates need to be set\n",
    "    \"end\":         \"2023040100\"    # this is likely the wrong end date\n",
    "}\n",
    "\n",
    "def request_pageviews_per_article(article_title = None, \n",
    "                                  endpoint_url = API_REQUEST_PAGEVIEWS_ENDPOINT, \n",
    "                                  endpoint_params = API_REQUEST_PER_ARTICLE_PARAMS, \n",
    "                                  request_template = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE,\n",
    "                                  headers = REQUEST_HEADERS):\n",
    "    '''\n",
    "    Description:\n",
    "        Used to request data for a single article, uses defined constants above for params\n",
    "    Input(s):\n",
    "        article_title - String\n",
    "        endpoint_url - String\n",
    "        endpoint_params - String\n",
    "        request_template - Dictionary\n",
    "        headers - Dictionary\n",
    "    Outputs:\n",
    "        json_response - JSON string/object\n",
    "    '''\n",
    "    # article title can be as a parameter to the call or in the request_template\n",
    "    if article_title:\n",
    "        request_template['article'] = article_title\n",
    "\n",
    "    if not request_template['article']:\n",
    "        raise Exception(\"Must supply an article title to make a pageviews request.\")\n",
    "\n",
    "    # Titles are supposed to have spaces replaced with \"_\" and be URL encoded\n",
    "    title_replaced = request_template['article'].replace(' ','_')\n",
    "    article_title_encoded = urllib.parse.quote(title_replaced)\n",
    "    request_template['article'] = article_title_encoded\n",
    "    article_title_encoded = (article_title_encoded.replace('/', '%2F'))\n",
    "    \n",
    "    # now, create a request URL by combining the endpoint_url with the parameters for the request\n",
    "    request_url = endpoint_url+endpoint_params.format(**request_template)\n",
    "    #print(request_url)\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7e81d",
   "metadata": {},
   "source": [
    "## Create Dataset1: Get all monthly mobile pageviews per article\n",
    "\n",
    "File will be named: academy_monthly_mobile_201501-202309.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ef928d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define param for mobile viewership from start date to current time\n",
    "ARTICLE_PAGEVIEWS_MOBILE_app = {\n",
    "    \"project\":     \"en.wikipedia.org\",\n",
    "    \"access\":      \"mobile-app\",      # this should be changed for the different access types\n",
    "    \"agent\":       \"user\",\n",
    "    \"article\":     \"\",             # this value will be set/changed before each request\n",
    "    \"granularity\": \"monthly\",\n",
    "    \"start\":       \"2015010100\",   # start and end dates need to be set\n",
    "    \"end\":         \"2023100100\"    # this is likely the wrong end date\n",
    "}\n",
    "ARTICLE_PAGEVIEWS_MOBILE_web = {\n",
    "    \"project\":     \"en.wikipedia.org\",\n",
    "    \"access\":      \"mobile-web\",      # this should be changed for the different access types\n",
    "    \"agent\":       \"user\",\n",
    "    \"article\":     \"\",             # this value will be set/changed before each request\n",
    "    \"granularity\": \"monthly\",\n",
    "    \"start\":       \"2015010100\",   # start and end dates need to be set\n",
    "    \"end\":         \"2023100100\"    # this is likely the wrong end date\n",
    "}\n",
    "\n",
    "#Store API results to dictionary, which will be written to file\n",
    "results = {}\n",
    "\n",
    "#Loop over each article of interest\n",
    "for article in ArticleInfo:\n",
    "    #Get mobile app and mobile web views\n",
    "    request1 = request_pageviews_per_article(article[0], request_template=ARTICLE_PAGEVIEWS_MOBILE_app)\n",
    "    request2 = request_pageviews_per_article(article[0], request_template=ARTICLE_PAGEVIEWS_MOBILE_web)\n",
    "    \n",
    "    #Store combined views for each month\n",
    "    inner_dictionary = {\"Views\": {}}\n",
    "    #print(article[0])\n",
    "    for i in range(len(request1['items'])):\n",
    "        \n",
    "        if(request1.__contains__(\"items\") and request2.__contains__(\"items\")):\n",
    "            month = request1['items'][i]['timestamp']\n",
    "            views = request1['items'][i]['views'] + request2['items'][i]['views']\n",
    "            inner_dictionary[\"Views\"][month] = views\n",
    "        elif(request2.__contains__(\"items\")):\n",
    "            month = request2['items'][i]['timestamp']\n",
    "            views = request2['items'][i]['views'] \n",
    "            inner_dictionary[\"Views\"][month] = views\n",
    "        elif(request1.__contains__(\"items\")):\n",
    "            month = request1['items'][i]['timestamp']\n",
    "            views = request1['items'][i]['views'] \n",
    "            inner_dictionary[\"Views\"][month] = views\n",
    "        else:\n",
    "            print(\"Missing data for\", article)\n",
    "            continue\n",
    "    \n",
    "    #Add to final dictionary\n",
    "    results[article[0]] = inner_dictionary\n",
    "\n",
    "#Sort dictionary by article name alphabetically descending\n",
    "results = sorted(results.items(), key=lambda x:x[0])\n",
    "\n",
    "#Write to file\n",
    "with open(\"../Data/academy_monthly_mobile_201501-202309.json\", \"w\") as outfile:\n",
    "    json.dump(results, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8159178d",
   "metadata": {},
   "source": [
    "## Create Dataset2: Get all monthly desktop pageviews per article\n",
    "\n",
    "File will be named: academy_monthly_desktop_201501-202309.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b1d0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define param for mobile viewership from start date to current time\n",
    "ARTICLE_PAGEVIEWS_desktop = {\n",
    "    \"project\":     \"en.wikipedia.org\",\n",
    "    \"access\":      \"desktop\",      # this should be changed for the different access types\n",
    "    \"agent\":       \"user\",\n",
    "    \"article\":     \"\",             # this value will be set/changed before each request\n",
    "    \"granularity\": \"monthly\",\n",
    "    \"start\":       \"2015010100\",   # start and end dates need to be set\n",
    "    \"end\":         \"2023100100\"    # this is likely the wrong end date\n",
    "}\n",
    "\n",
    "#Store API results to dictionary, which will be written to file\n",
    "results = {}\n",
    "\n",
    "#Loop over each article of interest\n",
    "for article in ArticleInfo:\n",
    "    #Get mobile app and mobile web views\n",
    "    request1 = request_pageviews_per_article(article[0], request_template=ARTICLE_PAGEVIEWS_desktop)\n",
    "    \n",
    "    if(request1.__contains__(\"items\")):\n",
    "        #Store combined views for each month\n",
    "        inner_dictionary = {\"Views\": {}}\n",
    "        for i in range(len(request1['items'])):\n",
    "            month = request1['items'][i]['timestamp']\n",
    "            views = request1['items'][i]['views']\n",
    "            inner_dictionary[\"Views\"][month] = views\n",
    "    else:\n",
    "        print(\"Missing data for\", article)\n",
    "        continue\n",
    "\n",
    "    #Store combined views for each month\n",
    "    #inner_dictionary = {\"Views\": {}}\n",
    "    #for i in range(len(request1['items'])):\n",
    "    #    month = request1['items'][i]['timestamp']\n",
    "    #    views = request1['items'][i]['views']\n",
    "    #    inner_dictionary[\"Views\"][month] = views\n",
    "    \n",
    "    #Add to final dictionary\n",
    "    results[article[0]] = inner_dictionary\n",
    "\n",
    "#Sort dictionary by article name alphabetically descending\n",
    "results = sorted(results.items(), key=lambda x:x[0])\n",
    "\n",
    "#Write to file\n",
    "with open(\"../Data/academy_monthly_desktop_201501-202309.json\", \"w\") as outfile:\n",
    "    json.dump(results, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6033be7",
   "metadata": {},
   "source": [
    "## Create Dataset3: Get all monthly desktop+mobile pageviews per article\n",
    "\n",
    "File will be named: academy_monthly_cumulative_201501-202309.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b48ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define param for mobile viewership from start date to current time\n",
    "ARTICLE_PAGEVIEWS_all = {\n",
    "    \"project\":     \"en.wikipedia.org\",\n",
    "    \"access\":      \"all-access\",      # this should be changed for the different access types\n",
    "    \"agent\":       \"user\",\n",
    "    \"article\":     \"\",             # this value will be set/changed before each request\n",
    "    \"granularity\": \"monthly\",\n",
    "    \"start\":       \"2015010100\",   # start and end dates need to be set\n",
    "    \"end\":         \"2023100100\"    # this is likely the wrong end date\n",
    "}\n",
    "\n",
    "#Store API results to dictionary, which will be written to file\n",
    "results = {}\n",
    "\n",
    "#Loop over each article of interest\n",
    "for article in ArticleInfo:\n",
    "    #Get mobile app and mobile web views\n",
    "    request1 = request_pageviews_per_article(article[0], request_template=ARTICLE_PAGEVIEWS_all)\n",
    "  \n",
    "    if(request1.__contains__(\"items\")):\n",
    "        #Store combined views for each month\n",
    "        inner_dictionary = {\"Views\": {}}\n",
    "        for i in range(len(request1['items'])):\n",
    "            month = request1['items'][i]['timestamp']\n",
    "            views = request1['items'][i]['views']\n",
    "            inner_dictionary[\"Views\"][month] = views\n",
    "    else:\n",
    "        print(\"Missing data for\", article)\n",
    "        continue\n",
    "    \n",
    "    #Add to final dictionary\n",
    "    results[article[0]] = inner_dictionary\n",
    "\n",
    "#Sort dictionary by article name alphabetically descending\n",
    "results = sorted(results.items(), key=lambda x:x[0])\n",
    "\n",
    "#Write to file\n",
    "with open(\"../Data/academy_monthly_cumulative_201501-202309.json\", \"w\") as outfile:\n",
    "    json.dump(results, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377cdd69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
